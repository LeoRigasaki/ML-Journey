{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Paper Study: Logistic Regression\n",
    "\n",
    "This notebook explores key research papers that shaped our understanding of Logistic Regression.\n",
    "\n",
    "## 1. Historical Development\n",
    "\n",
    "### 1.1 Original Work by Pierre Fran√ßois Verhulst (1838)\n",
    "- Paper: \"Notice sur la loi que la population poursuit dans son accroissement\"\n",
    "- First introduction of the logistic function\n",
    "- Application to population growth models\n",
    "- Key contributions:\n",
    "  - Introduced S-shaped curve (sigmoid function)\n",
    "  - Established mathematical foundation for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_verhulst_curve():\n",
    "    \"\"\"Plot the original logistic curve studied by Verhulst.\"\"\"\n",
    "    x = np.linspace(-10, 10, 1000)\n",
    "    y = 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(x, y, 'b-', label='Logistic Function')\n",
    "    plt.title('Verhulst\\'s Logistic Function (1838)')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('P(x)')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_verhulst_curve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Maximum Likelihood Estimation\n",
    "\n",
    "### 2.1 Fisher's Contribution (1935)\n",
    "- Paper: \"The Case of Zero Survivors in Probit Assays\"\n",
    "- Established MLE as the standard estimation method\n",
    "- Key insights:\n",
    "  - Proved optimality of MLE under certain conditions\n",
    "  - Introduced Fisher scoring method for optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(X, y, beta):\n",
    "    \"\"\"Compute log-likelihood as described in Fisher's paper.\"\"\"\n",
    "    z = np.dot(X, beta)\n",
    "    p = 1 / (1 + np.exp(-z))\n",
    "    return np.sum(y * np.log(p) + (1 - y) * np.log(1 - p))\n",
    "\n",
    "# Example usage will be added in implementation notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modern Developments\n",
    "\n",
    "### 3.1 Regularization Methods\n",
    "\n",
    "#### 3.1.1 Tibshirani (1996)\n",
    "- Paper: \"Regression Shrinkage and Selection via the Lasso\"\n",
    "- Introduced L1 regularization\n",
    "- Impact on logistic regression:\n",
    "  - Feature selection\n",
    "  - Sparse models\n",
    "  - Better generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_regularization(beta, lambda_):\n",
    "    \"\"\"L1 regularization term as described by Tibshirani.\"\"\"\n",
    "    return lambda_ * np.sum(np.abs(beta))\n",
    "\n",
    "def l2_regularization(beta, lambda_):\n",
    "    \"\"\"L2 regularization (Ridge) term.\"\"\"\n",
    "    return lambda_ * np.sum(beta**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Numerical Optimization Methods\n",
    "\n",
    "#### 3.2.1 Iteratively Reweighted Least Squares (IRLS)\n",
    "- Paper: Nelder & Wedderburn (1972)\n",
    "- \"Generalized Linear Models\"\n",
    "- Key contributions:\n",
    "  - Efficient optimization algorithm\n",
    "  - Connection to weighted least squares\n",
    "  - Quadratic convergence properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def irls_weights(p):\n",
    "    \"\"\"Compute IRLS weights as described by Nelder & Wedderburn.\"\"\"\n",
    "    return p * (1 - p)\n",
    "\n",
    "# Full IRLS implementation will be in implementation notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Recent Research Directions\n",
    "\n",
    "### 4.1 Handling Imbalanced Data\n",
    "- Paper: He & Garcia (2009)\n",
    "- \"Learning from Imbalanced Data\"\n",
    "- Key insights:\n",
    "  - Sampling strategies\n",
    "  - Cost-sensitive learning\n",
    "  - Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weights(y):\n",
    "    \"\"\"Compute balanced class weights as discussed in He & Garcia.\"\"\"\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    return {u: len(y)/(len(unique)*c) for u, c in zip(unique, counts)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Key Takeaways from Literature Review\n",
    "\n",
    "1. **Mathematical Foundation**:\n",
    "   - Strong theoretical basis from Verhulst and Fisher\n",
    "   - Proven optimality under certain conditions\n",
    "\n",
    "2. **Estimation Methods**:\n",
    "   - MLE is standard approach\n",
    "   - IRLS provides efficient computation\n",
    "   - Various regularization techniques available\n",
    "\n",
    "3. **Modern Challenges**:\n",
    "   - Handling imbalanced data\n",
    "   - Feature selection\n",
    "   - Scalability to large datasets\n",
    "\n",
    "4. **Future Research Directions**:\n",
    "   - Online learning variants\n",
    "   - Integration with deep learning\n",
    "   - Interpretability methods"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
